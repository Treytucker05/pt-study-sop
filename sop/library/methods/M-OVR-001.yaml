id: M-OVR-001
name: Exit Ticket
category: overlearn
description: 'Answer three questions: (1) What did I learn? (2) What''s still muddy?
  (3) What''s my next action?'
default_duration_min: 3
energy_cost: low
best_stage: consolidation
control_stage: OVERLEARN
status: validated
tags:
- reflection
- meta
- wrap
- sop-core
mechanisms:
- metacognition
- reflection
- calibration
- planning
inputs:
- Completed study session
- Exit ticket template (3 questions)
- Session notes or artifacts for reference
steps:
- step: 1
  action: 'Answer Question 1: ''What did I learn today?'''
  notes: Free recall format — blurt 3-5 key takeaways without looking at notes
- step: 2
  action: 'Answer Question 2: ''What''s still muddy?'''
  notes: Identify 1-2 specific confusions, gaps, or uncertain areas
- step: 3
  action: 'Answer Question 3: ''What''s my next action?'''
  notes: Concrete, specific next step — not vague ("review more")
- step: 4
  action: Review your blurt against session materials
  notes: Did you miss any major points? Add them if critical.
- step: 5
  action: Convert muddy points into specific questions for next session
  notes: Transform 'I'm confused about X' into 'How does X work when Y?'
- step: 6
  action: Log exit ticket to session record
  notes: This feeds into scheduling and gap tracking systems
outputs:
- Exit ticket (3-part response)
- Key takeaways list
- Muddy points / weak anchors
- Next action commitment
- Questions for next session
stop_criteria:
- All three questions answered
- Next action is specific and actionable
- Exit ticket logged to session record
failure_modes:
- mode: Generic answers ("I learned a lot")
  mitigation: Force specificity — name actual concepts, facts, or skills
- mode: Skipping the muddy point question
  mitigation: Something is always muddy — dig for it
- mode: Vague next action ("study more")
  mitigation: Next action must pass the "could I do this in 15 min?" test
logging_fields:
- takeaways_count
- muddy_points_identified
- next_action_specificity
- exit_ticket_completed
evidence:
  citation: Tanner (2012)
  finding: metacognitive reflection improves self-regulated learning
  source: seed_methods.py
evidence_raw: Tanner (2012); metacognitive reflection improves self-regulated learning
knobs:
  scaffolding_intensity: minimal_nudges
  feedback_style: strict_gap_analysis
  output_format: json_rubric
facilitation_prompt: You are a strict examiner conducting a retrieval practice drill.
  You must operate at a minimal_nudges level. Ask the student one question at a time.
  Evaluate their answer strictly. Provide a hint based on the scaffolding level, but
  NEVER reveal the final answer. Output a hidden JSON evaluation logging 'correct'
  or 'incorrect' for the system telemetry.
has_facilitation_prompt: Y
gates:
- rule: requires_reference_targets
  threshold: 0.95
  fallback_action: redirect_to_encode
